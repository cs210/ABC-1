{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Political Leaning and Emotion Detectors\n",
    "\n",
    "https://www.niemanlab.org/2020/01/republicans-and-democrats-live-in-nearly-inverse-news-media-environments-pew-finds/\n",
    "\n",
    "https://newsapi.org/\n",
    "\n",
    "\n",
    "We used a Pew Research study on partisan trust in news media sources to calculate each outlet's \"trust score.\" This is calculated by subtracting the percentage of democrats who trust a news source from the percentage of republicans who trust a news source, then multiplying by the sum of the percent of democrats and republicans who reported for that source.\n",
    "\n",
    "The values were then normalized and then adjusted to account for the fact that republican distrust among news media is higher than democrats, and divided to be a number scaled from -8, being the most extreme left, and +8, being the most extreme right.\n",
    "\n",
    "We will get articles and headlines from the News API that we're using for our UI. Because we don't have the developper version, we're limited to 100 articles per news source, which significantly limits the accuracy of our regression, and we can do more extensive analysis with the entire aritcle as opposed to just headlines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hitting the Sources API\n",
    "\n",
    "We took the sources from the Pew Research study, hit the sources API and recorded all of the API id's into a list to reference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = ('http://newsapi.org/v2/sources?'\n",
    "       'country=us&'\n",
    "       'apiKey=8e213af13b7645518701df89f5cdbecc')\n",
    "\n",
    "response = requests.get(url)\n",
    "pew_sources = ['ABC News',\n",
    "           'Breitbart News',\n",
    "           'Business Insider',\n",
    "           'CBS News',\n",
    "           'CNN',\n",
    "           'Fox News',\n",
    "           'The Hill',\n",
    "           'The Huffington Post',\n",
    "           'NBC News',\n",
    "           'Newsweek',\n",
    "           'Politico',\n",
    "           'MSNBC',\n",
    "           'Time',\n",
    "           'USA Today',\n",
    "           'Vice News',\n",
    "           'The Wall Street Journal',\n",
    "           'The Washington Post']\n",
    "\n",
    "pew_trust_scores = {\n",
    "    'ABC News': -0.93,\n",
    "    'Breitbart News': 2.51,\n",
    "    'Business Insider': 1.90,\n",
    "    'CBS News': -1.04,\n",
    "    'CNN': -3.51,\n",
    "    'Fox News': 7.60,\n",
    "    'The Hill': 1.90,\n",
    "    'The Huffington Post': 1.09,\n",
    "    'NBC News': -1.37,\n",
    "    'Newsweek': 0.91,\n",
    "    'Politico': -1.48,\n",
    "    'MSNBC': -2.90,\n",
    "    'Time': -0.21,\n",
    "    'USA Today': 1.00,\n",
    "    'Vice News': 1.73,\n",
    "    'The Wall Street Journal': 1.02,\n",
    "    'The Washington Post': -1.86\n",
    "}\n",
    "\n",
    "json = response.json()\n",
    "\n",
    "source_strs = []\n",
    "for source in json['sources']:\n",
    "    if source['name'] in pew_trust_scores:\n",
    "        source_strs += [source['id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Headlines\n",
    "\n",
    "Because political context is really important, we are only using training data in a one month time period and labeling each tweet with its trust score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "\n",
    "def results_from_source(curr_source):\n",
    "    last_date = (date.today()-timedelta(days=29)).isoformat()\n",
    "    page_num = '1'\n",
    "    num_results = 100\n",
    "    results = []\n",
    "    while True:\n",
    "        url = ('http://newsapi.org/v2/everything?'\n",
    "               'sources=' + curr_source + '&'\n",
    "               'pageSize=100&'\n",
    "               'page=' + page_num + '&'\n",
    "               'apiKey=8e213af13b7645518701df89f5cdbecc')\n",
    "        response = requests.get(url)\n",
    "        if 'articles' not in response.json():\n",
    "            break\n",
    "        results += response.json()['articles']\n",
    "        page_num = str(int(page_num) + 1)\n",
    "\n",
    "    return results\n",
    "\n",
    "all_results = []\n",
    "for source_name in source_strs:\n",
    "    all_results += results_from_source(source_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing and Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def tokenize(headline):\n",
    "    headline = headline.lower() #lower case\n",
    "    headline = re.sub(r'\\([0-9]/[0-9]\\)', '', headline, flags=re.MULTILINE)\n",
    "    headline = re.sub(r'[^A-Za-z0-9 ]', '', headline, flags=re.MULTILINE) # remove punctuation\n",
    "    headline = re.sub(r'  ', ' ', headline, flags=re.MULTILINE) # remove double spaces\n",
    "    tokens = tokenizer.tokenize(headline) #tokenize\n",
    "    if '' in tokens:\n",
    "        tokens.remove('')\n",
    "    return tokens\n",
    "    \n",
    "tokenized = []\n",
    "for article in all_results:\n",
    "    if article['title']:\n",
    "        tokenized.append([tokenize(article['title']), pew_trust_scores[article['source']['name']]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words\n",
    "\n",
    "Removing stop words that will add noise to our classifier using the NLTK stopwords package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('us')\n",
    "\n",
    "stop_words_removed = []\n",
    "for headline in tokenized:\n",
    "    filtered = [word for word in headline[0] if word not in stop_words]\n",
    "    stop_words_removed += [[' '.join(filtered), headline[1]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Data Set into Training and Testing\n",
    "\n",
    "We will randomly shuffle all the data and then split the train/test data 80/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_headlines = np.asarray(stop_words_removed)\n",
    "np.random.shuffle(processed_headlines)\n",
    "\n",
    "idx = 4 * len(processed_headlines) // 5\n",
    "\n",
    "train_x = processed_headlines[:idx, 0]\n",
    "train_y = processed_headlines[:idx, 1]\n",
    "\n",
    "test_x = processed_headlines[idx:, 0]\n",
    "test_y = processed_headlines[idx:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions to Extract N-Gram Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigram_data(x, y):\n",
    "    assert(len(x) == len(y))\n",
    "    unigram_x = []\n",
    "    unigram_y = []\n",
    "    for i in range(len(x)):\n",
    "        unigram_x.append(x[i])\n",
    "        unigram_y.append(y[i])\n",
    "\n",
    "    return np.asarray(unigram_x).reshape(-1, 1), np.asarray(unigram_y).astype(float).reshape(-1, 1)\n",
    "\n",
    "def get_bigram_data(x, y):\n",
    "    bigram_x = []\n",
    "    bigram_y = []\n",
    "    for i in range(len(x)):\n",
    "        words = x[i].split(' ')\n",
    "        if len(words) < 2:\n",
    "            continue\n",
    "            \n",
    "        bigrams = []\n",
    "        for j in range(0, len(words) - 1):\n",
    "            bigrams.append(words[j] + words[j + 1])\n",
    "            \n",
    "        bigram_x.append(' '.join(bigrams))\n",
    "        bigram_y.append(y[i])\n",
    "\n",
    "    return np.asarray(bigram_x).reshape(-1, 1), np.asarray(bigram_y).astype(float).reshape(-1, 1)\n",
    "\n",
    "\n",
    "def get_trigram_data(x, y):\n",
    "    trigram_x = []\n",
    "    trigram_y = []\n",
    "    for i in range(len(x)):\n",
    "        words = x[i].split(' ')\n",
    "        if len(words) < 3:\n",
    "            continue\n",
    "            \n",
    "        trigrams = []\n",
    "        for j in range(0, len(words) - 2):\n",
    "            trigrams.append(words[j] + words[j + 1] + words[j + 2])\n",
    "\n",
    "        trigram_x.append(' '.join(trigrams))\n",
    "        trigram_y.append(y[i])\n",
    "\n",
    "    return np.asarray(trigram_x).reshape(-1, 1), np.asarray(trigram_y).astype(float).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function to Convert Headlines into Sparse Vectors\n",
    "\n",
    "We may integrate this later should we choose to do this instead of encoding our strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_counts(train_x, test_x):\n",
    "    split_idx = len(train_x)\n",
    "    all_data = train_x + test_x\n",
    "\n",
    "    count_vect = CountVectorizer()\n",
    "    counts = count_vect.fit_transform(all_data)\n",
    "    feat_dict=count_vect.vocabulary_.keys()\n",
    "\n",
    "    x_train_counts = counts[:split_idx,:]\n",
    "    x_test_counts = counts[split_idx:,:]\n",
    "    \n",
    "    return x_train_counts, x_test_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Data and Perform KFold Cross Validation\n",
    "\n",
    "In order to predict the float output, we will encode the strings and floats and then decode them in our predictions.\n",
    "\n",
    "Moving forward I will likely try to simplify this process by changing trust scores to scaled integers from 0 to 100, so that we don't have to encode/decode. Also, I want to experiment with using bigram and trigram features on the headlines to see if that improves MSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karinaasanchez/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/karinaasanchez/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:273: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/karinaasanchez/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/karinaasanchez/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:273: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/karinaasanchez/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  35.62890625\n",
      "MSE:  43.859375\n",
      "MSE:  43.9140625\n",
      "MSE:  38.7734375\n",
      "MSE:  39.3125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karinaasanchez/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:273: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/karinaasanchez/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/karinaasanchez/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:273: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/karinaasanchez/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/karinaasanchez/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:273: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "for i in range(len(train_x)):\n",
    "    train_x[i:,] = le.fit_transform(train_x[i:,])\n",
    "    \n",
    "train_y = le.fit_transform(train_y)\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "nb_accuracies = []\n",
    "lr_accuracies = []\n",
    "\n",
    "for train_index, test_index in kf.split(train_x):\n",
    "    x_fold_train, x_fold_test = train_x[train_index], train_x[test_index]\n",
    "    y_fold_train, y_fold_test = train_y[train_index], train_y[test_index]\n",
    "\n",
    "    unigram_train_x, unigram_train_y = get_unigram_data(x_fold_train, y_fold_train)\n",
    "    unigram_test_x, unigram_test_y = get_unigram_data(x_fold_test, y_fold_test)\n",
    "\n",
    "    bigram_train_x, bigram_train_y = get_bigram_data(x_fold_train, y_fold_train)\n",
    "    bigram_test_x, bigram_test_y = get_bigram_data(x_fold_test, y_fold_test)\n",
    "\n",
    "    trigram_train_x, trigram_train_y = get_trigram_data(x_fold_train, y_fold_train)\n",
    "    trigram_test_x, trigram_test_y = get_trigram_data(x_fold_test, y_fold_test)\n",
    "    \n",
    "    svc_unigram = SVC(gamma='auto').fit(unigram_train_x, unigram_train_y)\n",
    "    unigram_predicted = svc_unigram.predict(unigram_test_x).astype(int)\n",
    "    decoded_pred = le.inverse_transform(unigram_predicted)\n",
    "    decoded_actual = le.inverse_transform(unigram_test_y.astype(int))\n",
    "    \n",
    "    print (\"MSE: \", np.sum((decoded_pred - decoded_actual)**2)/float(len(unigram_test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
